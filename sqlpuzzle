https://www.stratascratch.com/blog/types-of-window-functions-in-sql-and-questions-asked-by-airbnb-netflix-twitter-and-uber/

Aggregation
Ranking
Ntiles
Lag/lead

1: distance_per_dollar question Uber
https://platform.stratascratch.com/coding/10302-distance-per-dollar?code_type=1
You’re given a dataset of uber rides with the traveling distance (‘distance_to_travel’) and cost (‘monetary_cost’) for each ride. First, find the difference between the distance-per-dollar for each date and the average distance-per-dollar for that year-month. Distance-per-dollar is defined as the distance traveled divided by the cost of the ride. Use the calculated difference on each date to calculate absolute average difference in distance-per-dollar metric on monthly basis (year-month).


The output should include the year-month (YYYY-MM) and the absolute average difference in distance-per-dollar (Absolute value to be rounded to the 2nd decimal).
You should also count both success and failed request_status as the distance and cost values are populated for all ride requests. Also, assume that all dates are unique in the dataset. Order your results by earliest request date first.


You’re given a dataset of uber rides with the traveling distance (‘distance_to_travel’) and cost (‘monetary_cost’) for each ride. First, find the difference between the distance-per-dollar for each date and the average distance-per-dollar for that year-month. Distance-per-dollar is defined as the distance traveled divided by the cost of the ride. Use the calculated difference on each date to calculate absolute average difference in distance-per-dollar metric on monthly basis (year-month).

--The output should include the year-month (YYYY-MM) and the absolute average difference in distance-per-dollar (Absolute value to be rounded to the 2nd decimal).

You should also count both success and failed request_status as the distance and cost values are populated for all ride requests. Also, assume that all dates are unique in the dataset. Order your results by earliest request date first.

The demo sample query is wrong as he only calculated the avg function on the dist_per_cost in his first x min video, but he corrected the alias and calculation in final video.

Use window function can reduce sub-query usage, but still can work this out without using window function



/* Write your SQL here. Terminate each statement with a semicolon. */


    with base as
    (select request_date
    ,to_char(request_date,'YYYY-MM') as request_ym 
    ,distance_to_travel/monetary_cost as distance_per_dollar
    from uber_request_logs),
    agg as (
    select request_date
    ,distance_per_dollar
    ,avg(distance_per_dollar) over(partition by request_ym ) as avg_distance_per_dollar
    from base)
    select  request_date
    ,round(abs(distance_per_dollar-avg_distance_per_dollar)::decimal,2) as avg_diff
    from agg
    order by request_date
    ;
	

2:unique salary Twitter
Unique Salaries
Find the top three unique salaries for each department. Output the department name and the top 3 unique salaries by each department. Order your results alphabetically by department and then by highest salary to lowest.

https://platform.stratascratch.com/coding/9897-highest-salary-in-department?code_type=1 (the original problem in demo seems gone, this is the closest one)

usually in redshift, it's quick and no need to dedup first, but the demo solution provided in strata scratch used group by to dedup first, use dense_rank or rank depends on the request how to handle ties.
Three main functions can use to find the highest would be rank, dense_rank, row_number(when there is a tie, this is useful, it just return exact three rows if there are larger than three rows meet the criteria)
With base as (
Select distinct department 
,salary
,dense_rank() over (partition by department order by salary desc) as rank_salaries
From twitter_employee)
Select  department
,salary
From base 
Where rm_salaries <=3
Order by department, salary desc


3: fraud
https://platform.stratascratch.com/coding/10303-top-percentile-fraud?code_type=1
ABC Corp is a mid-sized insurer in the US and in the recent past their fraudulent claims have increased significantly for their personal auto insurance portfolio. They have developed a ML based predictive model to identify propensity of fraudulent claims. Now, they assign highly experienced claim adjusters for top 5 percentile of claims identified by the model.
Your objective is to identify the top 5 percentile of claims from each state. Your output should be policy number, state, claim cost, and fraud score.

NTILE N Bins
Ntile 100 - percentile
Ntile 4 —Quartile
Ntile 10 —Decile


With base as (
Select
policy_num
,state
,claim_cost
,fraud_score
,ntile(100) over (partition by state order by fraud_score desc) as percentile_num
From fraud_score )
Select distinct policy_num
,state
,claim_cost
,fraud_score
,percentile_num
From base where percentile_num <=5;



4:growth of Airbnb
https://platform.stratascratch.com/coding/9637-growth-of-airbnb?code_type=1

Estimate the growth of Airbnb each year using the number of hosts registered as the growth metric. The rate of growth is calculated by taking ((number of hosts registered in the current year - number of hosts registered in the previous year) / the number of hosts registered in the previous year) * 100.
Output the year, number of hosts in the current year, number of hosts in the previous year, and the rate of growth. Round the rate of growth to the nearest percent and order the result in the ascending order based on the year.
Assume that the dataset consists only of unique hosts, meaning there are no duplicate hosts listed.

--solution 1
with base as (
select to_char(host_since,'YYYY') as year
,count(id) as host_cnt
from airbnb_search_details
group by to_char(host_since,'YYYY'))
,
pre as (
select year
,host_cnt
,lag(host_cnt,1) over (order by year ) as pre_year_host_cnt
from base 
)
select *
--if it's blank, leave it as blank since cannot calculate the growth rate, but can double confirm the request
--,case when pre_year_host_cnt is null then 100 else round((1.00* (host_cnt-pre_year_host_cnt)/(pre_year_host_cnt) * 100)) end as growth_rate
--use 1.00 and cast would result in same result
--round(((host_cnt-pre_year_host_cnt)/cast(pre_year_host_cnt as numeric)) * 100) end as growth_rate
,round((1.00* (host_cnt-pre_year_host_cnt)/(pre_year_host_cnt) * 100))  as growth_rate
from pre
order by year
;

--solution 2:
--can also also use extract function and self join if requires to not use window function like lag
WITH base AS (
    SELECT 
        EXTRACT(YEAR FROM host_since) AS year,
        COUNT(id) AS host_cnt
    FROM 
        airbnb_search_details
    GROUP BY 
        EXTRACT(YEAR FROM host_since)
)
, pre AS (
    SELECT 
        b1.year AS year,
        b1.host_cnt AS host_cnt,
        b2.host_cnt AS pre_year_host_cnt
    FROM 
        base b1
    LEFT JOIN base b2 ON b1.year = b2.year + 1
)
SELECT 
    year,
    host_cnt,
    pre_year_host_cnt,
    CASE
        WHEN pre_year_host_cnt IS NULL THEN NULL
        ELSE ROUND((1.00* (host_cnt - pre_year_host_cnt) / pre_year_host_cnt::NUMERIC) * 100)
    END AS growth_rate
FROM 
    pre
ORDER BY 
    year;



Freestyle try:

Revenue Over Time Amazon
https://platform.stratascratch.com/coding/2148-monthly-sales-rolling-average?code_type=1

Find the 3-month rolling average of total revenue from purchases given a table with users, their purchase amount, and date purchased. Do not include returns which are represented by negative purchase values. Output the year-month (YYYY-MM) and 3-month rolling average of revenue, sorted from earliest month to latest month.
A 3-month rolling average is defined by calculating the average total revenue from all user purchases for the current month and previous two months. The first two months will not be a true 3-month rolling average since we are not given data from last year. Assume each month has at least one purchase.

with base as (
select to_char(created_at,'YYYY-MM') AS year_month
,sum(purchase_amt) as purchase_amt
from amazon_purchases
where purchase_amt >0
group by to_char(created_at,'YYYY-MM')
)
select  year_month
--,avg(purchase_amt) over (order by year_month range between interval '2 months' preceding and current row) as rolling_3_month_avg
,avg(purchase_amt) over (order by year_month asc rows between 2 preceding and current row) as rolling_3_month_avg
from base
order by year_month
;


with base as (
select date_trunc('month', created_at) AS year_month
,sum(purchase_amt) as purchase_amt
from amazon_purchases
where purchase_amt >0
group by date_trunc('month', created_at)
)
select to_char(year_month,'YYYY-MM') as year_month
,avg(purchase_amt) over (order by year_month range between interval '2 months' preceding and current row) as rolling_3_month_avg
from base
order by year_month
;



Monthly Sales Rolling Average Amazon
https://platform.stratascratch.com/coding/2148-monthly-sales-rolling-average?code_type=1
You have been asked to calculate the rolling average for book sales in 2022.


A rolling average continuously updates a data set's average to include all data in the set up to that point. For example, the rolling average for February would be calculated by adding the book sales from January and February and dividing by two; the rolling average for March would be calculated by adding the book sales from January, February, and March and dividing by three; and so on.


Output the month, the sales for that month, and an extra column containing the rolling average rounded to the nearest whole number.
with base as 
(select 
to_char(bo.order_date,'YYYY-MM') as year_month
,sum(unit_price * quantity) as book_sales
from book_orders bo
left join amazon_books ab on ab.book_id =  bo.book_id
group by to_char(order_date,'YYYY-MM')
),
agg as (
select year_month
,book_sales
,avg(book_sales) over (order by year_month asc rows between unbounded preceding and current row) as avg_rollover_sales
from base
)
select year_month
,book_sales
,round(avg_rollover_sales,0) as avg_rollover_sales
from agg
order by year_month
;


https://learnsql.com/blog/sql-window-functions-rows-clause/
ROWS BETWEEN lower_bound AND upper_bound

The bounds can be any of these five options:

UNBOUNDED PRECEDING – All rows before the current row.
n PRECEDING – n rows before the current row.
CURRENT ROW – Just the current row.
n FOLLOWING – n rows after the current row.
UNBOUNDED FOLLOWING – All rows after the current row.

running total sample --not stratascratch puzzle
SELECT date, revenue,
    SUM(revenue) OVER (
      ORDER BY date
      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) running_total
FROM sales
ORDER BY date;


--Year Over Year Churn 
Table: lyft_drivers

Find how the number of drivers that have churned changed in each year compared to the previous one. Output the year (specifically, you can use the year the driver left Lyft) 
along with the corresponding number of churns in that year, the number of churns in the previous year, and an indication on whether the number has been increased (output the value 'increase'), 
decreased (output the value 'decrease') or stayed the same (output the value 'no change').



with base as (
  select to_char(end_date,'YYYY') as year 
  ,count(distinct index) as curr_driver_churn
  from lyft_drivers
  where end_date is not null
  group by to_char(end_date,'YYYY')
),
lag_base as (
select year
,curr_driver_churn
,lag(curr_driver_churn,1) over (order by year) as prev_yr_driver_churn
from base
order by year)
select 
year
,COALESCE(curr_driver_churn,0) as curr_driver_churn
,COALESCE(prev_yr_driver_churn ,0) as prev_yr_driver_churn
,case when curr_driver_churn > COALESCE(prev_yr_driver_churn ,0) then 'increase' 
      when curr_driver_churn < COALESCE(prev_yr_driver_churn ,0)  then 'decrease' 
      when curr_driver_churn = COALESCE(prev_yr_driver_churn ,0)  then 'no change' end as bucket
      from lag_base
      order by year desc
;


--Strata Scratch other exercise
Users By Average Session Time facebook

https://platform.stratascratch.com/coding/10352-users-by-avg-session-time?code_type=1

Calculate each user's average session time. A session is defined as the time difference between a page_load and page_exit. For simplicity, assume a user has only 1 session per day and
if there are multiple of the same events on that day, consider only the latest page_load and earliest page_exit, with an obvious restriction that load time event should happen before exit time event . Output the user_id and their average session time.

with base as (
select date(timestamp) as date
,user_id
,max(timestamp) as page_load_time
from facebook_web_log
where action = 'page_load'
group by  date(timestamp) ,user_id)
,page_exist as
(select date(fwl.timestamp) as date
,fwl.user_id
,page_load_time
,min(case when fwl.timestamp > page_load_time then fwl.timestamp end) as page_exit_time
from facebook_web_log fwl
left join base b on date( fwl.timestamp) = b.date and fwl.user_id = b.user_id 
where fwl.action = 'page_exit'
group by date(fwl.timestamp),fwl.user_id,b.page_load_time)
select user_id
,avg(page_exit_time::timestamp - page_load_time::timestamp) as avg_session
from page_exist
group by 1
order by user_id
;


--https://medium.com/@ART2024/netflix-sql-interview-questions-e0132a17f323
netflix puzzle, but the answer in the doc are not 100% correct


--Doordash

https://platform.stratascratch.com/coding/2113-extremely-late-delivery?code_type=1
A delivery is flagged as extremely late if the actual delivery time is more than 20 minutes (not inclusive) after the predicted delivery time.
You have been asked to calculate the percentage of orders that arrive extremely late each month.
Your output should include the month in the format 'YYYY-MM' and the percentage of extremely late orders as a percentage of all orders placed in that month.

--explorer data so include extra columns needed
--clarify question like cancel order etc
with base as (
select to_char(order_placed_time,'YYYY-MM') as month
,actual_delivery_time
,predicted_delivery_time
,actual_delivery_time - predicted_delivery_time as gap
,case when EXTRACT(epoch from (actual_delivery_time - predicted_delivery_time)) > 1200 then 'Y' else 'N' end as late_deliver_flag
,case when EXTRACT(minute from (actual_delivery_time - predicted_delivery_time)) > 20 then 'Y' else 'N' end as late_deliver_min_flag
,delivery_id, consumer_id from delivery_orders)
select month
,1.00 * count(case when late_deliver_flag='Y' then delivery_id end) / count(delivery_id) as late_order_percentage
,1.00 * count(case when late_deliver_min_flag='Y' then delivery_id end) / count(delivery_id) as late_order_min_percentage
from base 
group by 1 order by 1
;


month	late_order_percentage	late_order_min_percentage
2021-11	0.308	0.308
2021-12	0.125	0.125
2022-01	0.364	0.364

instructions: https://www.stratascratch.com/blog/doordash-sql-interview-questions/
Imagine you’re a data analyst at DoorDash, and Management wants to know, for every month, what percentage of their orders are ‘extremely late deliveries’. A delivery is considered ‘extremely late’ if the order is delivered more than 20 minutes late of the predicted delivery time.

Intuitively, we would monitor the number of ‘extremely late’ deliveries month on month. However, we cannot look at this number alone. The total deliveries will change monthly and fluctuate based on season, holidays, and due to the general growth of the company. A better metric to track is the percentage of total orders that are ‘extremely late’. This is exactly what this DoorDash SQL interview question is asking us to do.

Ideally, we want to see this metric lower over time or at least, be kept within a certain threshold. Otherwise, this is a cause for concern, leading to low ratings, less repeat business, and poor food quality. Sooner than you think, the company is out of business. And you’re out of your job!

Using a framework for that will help you structure your thoughts and communicate them in a way that the interviewer can follow. This is important because oftentimes, it is not only your coding skills that are being tested but your communication skills as well.

The framework we recommend consists of the following steps.
Explore and understand the dataset
Confirm and state your assumptions
Outline your high-level approach in English or pseudo-code
Code the solution
Code optimizations (if there are any)

Once an order is placed, it is assigned a unique delivery ID, and the relevant details about the order like the customer, the restaurant, the date, and time of order are then logged. 
The delivery time/expected time of arrival of delivery is computed, and the order is assigned to a dasher/rider whose ID is also included in the table. 
Once the order is delivered, the exact time is logged, and the customer provides their feedback through a rating.

doordash

https://platform.stratascratch.com/coding/2036-lowest-revenue-generated-restaurants?code_type=1
Lowest Revenue Generated Restaurants
Write a query that returns a list of the bottom 2% revenue generating restaurants. Return a list of restaurant IDs and their total revenue from when customers placed orders in May 2020.
You can calculate the total revenue by summing the order_total column. And you should calculate the bottom 2% by partitioning the total revenue into evenly distributed buckets.


--can clarify whether we need to take discount, refund into account.
--the choice between using ntile(100) with num <= 2 and ntile(50) with num = 1 depends on the total number of records. In my tests on Redshift, if the dataset exceeds 50 records, both methods yield the same result. 
However, when the dataset contains fewer than 50 records, ntile(50) with num = 1 returns only one record, while ntile(100) with num <= 2 still returns two records. Therefore, it's better to clarify with the interviewer regarding the expected outcome, especially since it's not feasible to evenly distribute the data into buckets in such scenarios. Considering that the result cannot be fractional, returning one row might be preferable. However, better to communicate and discuss these considerations.
Or even use count(*) to decide what would be the proper n to use in ntile
mock up query:
CREATE  TABLE schema.doordash_delivery_new (
    restaurant_id INT,
    order_total DECIMAL(10, 2),
    customer_placed_order_datetime TIMESTAMP
);

truncate schema.doordash_delivery_new;
select count(*) from schema.doordash_delivery_new;

-- Inserting sample data
INSERT INTO schema.doordash_delivery_new (restaurant_id, order_total, customer_placed_order_datetime)
VALUES
    (1, 150.25, '2020-05-01 08:15:00'),
    (2, 250.50, '2020-05-01 12:30:00'),
    (3, 180.75, '2020-05-02 10:45:00'),
    (4, 300.00, '2020-05-02 15:00:00'),
    (5, 200.25, '2020-05-03 11:20:00'),
    (6, 400.50, '2020-05-03 18:45:00'),
    (7, 220.75, '2020-05-04 09:00:00'),
    (8, 350.00, '2020-05-04 14:30:00'),
    (9, 180.25, '2020-05-05 13:10:00'),
    (10, 280.50, '2020-05-05 19:45:00')
    /* ,(11, 310.75, '2020-05-06 08:20:00'),
    (12, 420.00, '2020-05-06 12:45:00'),
    (13, 240.25, '2020-05-07 11:30:00'),
    (14, 380.50, '2020-05-07 18:00:00'),
    (15, 190.75, '2020-05-08 10:00:00'),
    (16, 290.00, '2020-05-08 15:20:00'),
    (17, 220.25, '2020-05-09 09:30:00'),
    (18, 350.50, '2020-05-09 16:45:00'),
    (19, 260.75, '2020-05-10 08:45:00'),
    (20, 390.00, '2020-05-10 13:55:00')
    ,(21, 200.25, '2020-05-11 11:10:00'),
    (22, 310.50, '2020-05-11 17:30:00'),
    (23, 230.75, '2020-05-12 10:20:00'),
    (24, 340.00, '2020-05-12 15:40:00'),
    (25, 210.25, '2020-05-13 09:15:00'),
    (26, 320.50, '2020-05-13 14:25:00'),
    (27, 180.75, '2020-05-14 08:30:00'),
    (28, 290.00, '2020-05-14 12:55:00'),
    (29, 240.25, '2020-05-15 10:40:00'),
    (30, 350.50, '2020-05-15 17:20:00'),
    (31, 270.75, '2020-05-16 09:50:00'),
    (32, 390.00, '2020-05-16 16:15:00'),
    (33, 180.25, '2020-05-17 08:00:00'),
    (34, 290.50, '2020-05-17 13:30:00'),
    (35, 220.75, '2020-05-18 11:25:00'),
    (36, 330.00, '2020-05-18 18:40:00'),
    (37, 250.25, '2020-05-19 10:20:00'),
    (38, 370.50, '2020-05-19 15:55:00'),
    (39, 200.75, '2020-05-20 08:45:00'),
    (40, 310.00, '2020-05-20 14:10:00'),
    (41, 240.25, '2020-05-21 09:30:00'),
    (42, 350.50, '2020-05-21 17:00:00'),
    (43, 270.75, '2020-05-22 08:20:00'),
    (44, 390.00, '2020-05-22 12:45:00'),
    (45, 210.25, '2020-05-23 10:35:00'),
    (46, 320.50, '2020-05-23 16:10:00'),
    (47, 180.75, '2020-05-24 09:45:00'),
    (48, 290.00, '2020-05-24 15:20:00'),
    (49, 240.25, '2020-05-25 10:00:00'),
    (50, 350.50, '2020-05-25 17:35:00')
     ,(51, 270.75, '2020-05-26 08:30:00'),
    (52, 390.00, '2020-05-26 13:55:00'),
    (53, 200.25, '2020-05-27 11:20:00'),
    (54, 310.50, '2020-05-27 18:00:00'),
    (55, 230.75, '2020-05-28 09:15:00'),
    (56, 340.00, '2020-05-28 14:45:00'),
    (57, 220.25, '2020-05-29 10:30:00'),
    (58, 330.50, '2020-05-29 17:10:00'),
    (59, 250.75, '2020-05-30 09:40:00')
 , (60, 370.00, '2020-05-30 15:25:00'),
    (61, 190.25, '2020-05-31 08:20:00'),
    (62, 300.50, '2020-05-31 12:50:00'),
    (63, 240.75, '2020-05-01 13:10:00'),
    (64, 360.00, '2020-05-01 18:30:00'),
    (65, 280.25, '2020-05-02 11:25:00'),
    (66, 390.50, '2020-05-02 17:40:00'),
    (67, 210.75, '2020-05-03 08:45:00'),
    (68, 320.00, '2020-05-03 14:20:00'),
    (69, 250.25, '2020-05-04 10:00:00'),
    (70, 370.50, '2020-05-04 16:35:00'),
    (71, 190.75, '2020-05-05 09:30:00'),
    (72, 300.00, '2020-05-05 15:15:00'),
    (73, 230.25, '2020-05-06 11:40:00'),
    (74, 340.50, '2020-05-06 18:00:00'),
    (75, 260.75, '2020-05-07 08:20:00'),
    (76, 380.00, '2020-05-07 14:50:00'),
    (77, 200.25, '2020-05-08 10:10:00'),
    (78, 310.50, '2020-05-08 16:30:00'),
    (79, 240.75, '2020-05-09 09:15:00'),
    (80, 350.00, '2020-05-09 14:40:00'),
    (81, 270.25, '2020-05-10 11:20:00'),
    (82, 390.50, '2020-05-10 17:45:00'),
    (83, 210.75, '2020-05-11 09:30:00'),
    (84, 320.00, '2020-05-11 15:00:00'),
    (85, 250.25, '2020-05-12 10:10:00'),
    (86, 370.50, '2020-05-12 16:25:00'),
    (87, 190.75, '2020-05-13 08:40:00'),
    (88, 300.00, '2020-05-13 14:20:00'),
    (89, 230.25, '2020-05-14 09:25:00'),
    (90, 340.50, '2020-05-14 17:10:00')
   , (91, 260.75, '2020-05-15 10:20:00'),
    (92, 380.00, '2020-05-15 15:50:00'),
    (93, 200.25, '2020-05-16 11:15:00'),
    (94, 310.50, '2020-05-16 17:30:00'),
    (95, 240.75, '2020-05-17 08:25:00'),
    (96, 350.00, '2020-05-17 14:45:00'),
    (97, 270.25, '2020-05-18 10:00:00'),
    (98, 390.50, '2020-05-18 16:20:00'),
    (99, 210.75, '2020-05-19 09:10:00'),
    (100, 320.00, '2020-05-19 15:40:00') */
    ;


    select count(*) from schema.doordash_delivery_new;


with base as (
select restaurant_id, sum(order_total) as tot_revenue
from doordash_delivery 
--where to_char(customer_placed_order_datetime,'YYYY-MM') = '2020-05'
where customer_placed_order_datetime between '2020-05-01' and '2020-05-31'
group by 1),
rank_percent as (
select restaurant_id,tot_revenue
,ntile(100) over (order by tot_revenue) as percentile_num
from base
)
select *
from rank_percent
where percentile_num <=2
order by tot_revenue
;
restaurant_id	tot_revenue	percentile_num
90	14.16	1
26	14.65	2

or use ntile(50) and choose the bottom 1
with base as (
select restaurant_id, sum(order_total) as tot_revenue
from doordash_delivery 
--where to_char(customer_placed_order_datetime,'YYYY-MM') = '2020-05'
where customer_placed_order_datetime between '2020-05-01' and '2020-05-31'
group by 1),
rank_percent as (
select restaurant_id,tot_revenue
,ntile(50) over (order by tot_revenue) as percentile_num
from base
)
select *
from rank_percent
where percentile_num =1
order by tot_revenue
;

restaurant_id	tot_revenue	percentile_num
90	14.16	1
26	14.65	1


Doordash

https://platform.stratascratch.com/coding/2034-avg-earnings-per-weekday-and-hour?code_type=1

Avg Earnings per Weekday and Hour
You have been asked to calculate the average earnings per order segmented by a combination of weekday (all 7 days) and hour using the column customer_placed_order_datetime.

You have also been told that the column order_total represents the gross order total for each order. Therefore, you'll need to calculate the net order total.
The gross order total is the total of the order before adding the tip and deducting the discount and refund.
Note: In your output, the day of the week should be represented in text format (i.e., Monday). Also, round earnings to 2 decimals



select to_char(customer_placed_order_datetime,'Day')||extract(hour from customer_placed_order_datetime)::varchar as weektime_hour 
,round(avg(order_total + tip_amount - discount_amount::float - refunded_amount )::numeric,2) 
as gross_earning_avg
from doordash_delivery 
group by to_char(customer_placed_order_datetime,'Day')||extract(hour from customer_placed_order_datetime)::varchar
order by to_char(customer_placed_order_datetime,'Day')||extract(hour from customer_placed_order_datetime)::varchar;
